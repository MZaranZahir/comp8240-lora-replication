import argparse, os, random
import numpy as np
import torch

from datasets import load_dataset
from transformers import (AutoTokenizer, AutoModelForSequenceClassification,
                          DataCollatorWithPadding, Trainer, TrainingArguments)
from peft import LoraConfig, get_peft_model
from sklearn.metrics import accuracy_score, f1_score

def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

DATASETS = {
    "sst2": {"hf_path": ("glue", "sst2")},
    "imdb": {"hf_path": ("imdb",)},
    "ag_news": {"hf_path": ("ag_news",)},
    "trec": {"hf_path": ("trec",)}
}

def load_task(dataset_name: str):
    if dataset_name not in DATASETS:
        raise ValueError(f"Unsupported dataset: {dataset_name}. Choose from {list(DATASETS.keys())}")
    path = DATASETS[dataset_name]["hf_path"]
    ds = load_dataset(*path)

    # Standardize columns: text_col, label_col
    text_col = None
    for cand in ["sentence", "text"]:
        if cand in ds["train"].column_names:
            text_col = cand
            break
    if text_col is None:
        # attempt to find any string column
        for c in ds["train"].column_names:
            if ds["train"][c] and isinstance(ds["train"][c][0], str):
                text_col = c; break
    if text_col is None:
        raise ValueError("Could not find text column.")

    label_col = None
    for cand in ["label", "coarse_label"]:
        if cand in ds["train"].column_names:
            label_col = cand
            break
    if label_col is None:
        raise ValueError("Could not find label column.")

    # For glue sst2 test split is 'validation'
    test_split = "test" if "test" in ds else "validation"

    num_labels = len(set(ds["train"][label_col]))
    return ds, text_col, label_col, test_split, num_labels

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", type=str, default="sst2", choices=["sst2","imdb","ag_news","trec"])
    parser.add_argument("--model_name", type=str, default="roberta-base")
    parser.add_argument("--rank", type=int, default=8)
    parser.add_argument("--alpha", type=float, default=16.0)
    parser.add_argument("--dropout", type=float, default=0.05)
    parser.add_argument("--lr", type=float, default=2e-5)
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--epochs", type=int, default=3)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--output_dir", type=str, default="results/lora_out")
    args = parser.parse_args()

    set_seed(args.seed)

    ds, text_col, label_col, test_split, num_labels = load_task(args.dataset)

    tokenizer = AutoTokenizer.from_pretrained(args.model_name, use_fast=True)
    def tokenize(batch):
        return tokenizer(batch[text_col], truncation=True, max_length=256)

    ds = ds.map(tokenize, batched=True)
    ds = ds.rename_column(label_col, "labels")
    keep_cols = ["input_ids", "attention_mask", "labels"]
    ds = ds.remove_columns([c for c in ds["train"].column_names if c not in keep_cols])

    model = AutoModelForSequenceClassification.from_pretrained(args.model_name, num_labels=num_labels)

    lora_cfg = LoraConfig(
        r=args.rank,
        lora_alpha=int(args.alpha),
        lora_dropout=args.dropout,
        bias="none",
        task_type="SEQ_CLS",
        target_modules=["query", "value"]
    )
    model = get_peft_model(model, lora_cfg)

    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    def compute_metrics(eval_pred):
        logits, labels = eval_pred
        preds = np.argmax(logits, axis=-1)
        acc = accuracy_score(labels, preds)
        f1 = f1_score(labels, preds, average="macro")
        return {"accuracy": acc, "f1": f1}

    output_dir = os.path.join(args.output_dir, f"{args.dataset}_r{args.rank}")
    os.makedirs(output_dir, exist_ok=True)

    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        learning_rate=args.lr,
        num_train_epochs=args.epochs,
        weight_decay=0.01,
        logging_steps=50,
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="accuracy",
        report_to=[]
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=ds["train"],
        eval_dataset=ds["validation"] if "validation" in ds else ds[test_split],
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics
    )

    trainer.train()

    metrics = trainer.evaluate(ds[test_split])
    print(metrics)

    trainer.save_model(output_dir)
    with open(os.path.join(output_dir, "metrics.json"), "w") as f:
        import json
        json.dump({k: float(v) for k, v in metrics.items()}, f, indent=2)

    import pandas as pd
    row = {
        "dataset": args.dataset,
        "rank": args.rank,
        "accuracy": metrics.get("eval_accuracy", None),
        "f1": metrics.get("eval_f1", None),
        "trainable_params_pct": 0.9
    }
    pd.DataFrame([row]).to_csv(os.path.join(output_dir, "summary.csv"), index=False)
    print(f"Saved adapter and metrics to {output_dir}")

if __name__ == "__main__":
    main()
