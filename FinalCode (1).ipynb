{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Parameter-Efficient Fine-Tuning with LoRA: Replication and Extension\n",
        "\n",
        "**Course:** COMP8240 ‚Äì Final Submission  \n",
        "**Author:** Muhammad Zaran Zahir (47997222)  \n",
        "**Notebook Purpose:** Replication of LoRA (SST-2) + Extensions (IMDB, AG News, IMDB-Mini)\n",
        "\n",
        "This notebook reproduces the key results of *Hu et al. (2022), ‚ÄúLoRA: Low-Rank Adaptation of Large Language Models‚Äù* and extends them to new datasets.  \n",
        "We use `roberta-base` with LoRA adapters on the attention **query/value** projections, training **<1%** of parameters.\n"
      ],
      "metadata": {
        "id": "sawJ1aCV83Ll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If running in Colab and you need to install packages, uncomment:\n",
        "!pip install -q torch transformers datasets peft accelerate scikit-learn evaluate sentencepiece matplotlib pandas tqdm\n",
        "\n",
        "import torch, transformers, datasets, peft, sklearn, accelerate\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"datasets:\", datasets.__version__)\n",
        "print(\"peft:\", peft.__version__)\n",
        "print(\"scikit-learn:\", sklearn.__version__)\n",
        "print(\"accelerate:\", accelerate.__version__)\n"
      ],
      "metadata": {
        "id": "37HF6mZy87Tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, random, pathlib, numpy as np, pandas as pd\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    model_name: str = \"roberta-base\"\n",
        "    max_length: int = 256\n",
        "    batch_size: int = 16\n",
        "    epochs: int = 3\n",
        "    lr: float = 2e-5\n",
        "    rank: int = 8\n",
        "    alpha: int = 16\n",
        "    dropout: float = 0.05\n",
        "    seed: int = 42\n",
        "    out_dir: str = \"results/lora_out\"\n",
        "\n",
        "CFG = Config()\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    import torch\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "pathlib.Path(CFG.out_dir).mkdir(parents=True, exist_ok=True)\n",
        "set_seed(CFG.seed)\n"
      ],
      "metadata": {
        "id": "1jidkUTn9DoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Mapping: dataset -> (HF path), text col fallback order, label col fallback order, test split name\n",
        "DATASETS = {\n",
        "    \"sst2\":   { \"hf\": (\"glue\",\"sst2\"),    \"text\": [\"sentence\",\"text\"], \"label\": [\"label\"],           \"test\": \"validation\" },\n",
        "    \"imdb\":   { \"hf\": (\"imdb\",),          \"text\": [\"text\",\"sentence\"], \"label\": [\"label\"],           \"test\": \"test\" },\n",
        "    \"ag_news\":{ \"hf\": (\"ag_news\",),       \"text\": [\"text\",\"sentence\"], \"label\": [\"label\"],           \"test\": \"test\" },\n",
        "}\n",
        "\n",
        "def get_columns(ds, pref_list):\n",
        "    for c in pref_list:\n",
        "        if c in ds[\"train\"].column_names:\n",
        "            return c\n",
        "    # fallback to first string column\n",
        "    for c in ds[\"train\"].column_names:\n",
        "        if isinstance(ds[\"train\"][c][0], str):\n",
        "            return c\n",
        "    raise ValueError(\"Suitable column not found.\")\n",
        "\n",
        "def load_task(name):\n",
        "    meta = DATASETS[name]\n",
        "    ds = load_dataset(*meta[\"hf\"])\n",
        "    text_col  = get_columns(ds, meta[\"text\"])\n",
        "    label_col = get_columns(ds, meta[\"label\"])\n",
        "    test_split = meta[\"test\"] if meta[\"test\"] in ds else \"validation\"\n",
        "    num_labels = len(set(ds[\"train\"][label_col]))\n",
        "    return ds, text_col, label_col, test_split, num_labels\n"
      ],
      "metadata": {
        "id": "jlvrtaHs9Gqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
        "                          DataCollatorWithPadding, Trainer, TrainingArguments)\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def run_lora_experiment(dataset_name, rank=8, epochs=3, lr=2e-5, batch_size=16, max_length=256):\n",
        "    set_seed(CFG.seed)\n",
        "    ds, text_col, label_col, test_split, num_labels = load_task(dataset_name)\n",
        "\n",
        "    tok = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
        "    def tokenize(batch):\n",
        "        return tok(batch[text_col], truncation=True, max_length=max_length)\n",
        "    ds = ds.map(tokenize, batched=True)\n",
        "    ds = ds.rename_column(label_col, \"labels\")\n",
        "    keep = [\"input_ids\",\"attention_mask\",\"labels\"]\n",
        "    ds = ds.remove_columns([c for c in ds[\"train\"].column_names if c not in keep])\n",
        "\n",
        "    # Limit training samples for faster runs (reduced as hardware limitations)\n",
        "    if dataset_name == \"sst2\":  # only apply for replication test, change as desired\n",
        "        ds[\"train\"] = ds[\"train\"].select(range(5000))\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name, num_labels=num_labels)\n",
        "\n",
        "    lora_cfg = LoraConfig(\n",
        "        r=rank, lora_alpha=CFG.alpha, lora_dropout=CFG.dropout,\n",
        "        bias=\"none\", task_type=\"SEQ_CLS\",\n",
        "        target_modules=[\"query\",\"value\"]\n",
        "    )\n",
        "    model = get_peft_model(model, lora_cfg)\n",
        "\n",
        "    collator = DataCollatorWithPadding(tokenizer=tok)\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        preds = np.argmax(logits, axis=-1)\n",
        "        acc = accuracy_score(labels, preds)\n",
        "        f1  = f1_score(labels, preds, average=\"macro\")\n",
        "        return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "    out = os.path.join(CFG.out_dir, f\"{dataset_name}_r{rank}\")\n",
        "    os.makedirs(out, exist_ok=True)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "    output_dir=out,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    eval_strategy=\"epoch\",        #  was evaluation_strategy\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=lr,\n",
        "    num_train_epochs=epochs,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    report_to=[]                  # disable W&B etc.\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model, args=args,\n",
        "        train_dataset=ds[\"train\"],\n",
        "        eval_dataset=ds[\"validation\"] if \"validation\" in ds else ds[test_split],\n",
        "        tokenizer=tok, data_collator=collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "    trainer.train()\n",
        "\n",
        "    metrics = trainer.evaluate(ds[test_split])\n",
        "    print(f\"[{dataset_name}] test metrics:\", metrics)\n",
        "\n",
        "    # Save adapter + metrics + summary row for report\n",
        "    trainer.save_model(out)\n",
        "    with open(os.path.join(out, \"metrics.json\"), \"w\") as f:\n",
        "        json.dump({k: float(v) for k,v in metrics.items()}, f, indent=2)\n",
        "\n",
        "    row = {\n",
        "        \"dataset\": dataset_name,\n",
        "        \"rank\": rank,\n",
        "        \"accuracy\": metrics.get(\"eval_accuracy\"),\n",
        "        \"f1\": metrics.get(\"eval_f1\"),\n",
        "        \"trainable_params_pct\": 0.9  # informative placeholder for report\n",
        "    }\n",
        "    pd.DataFrame([row]).to_csv(os.path.join(out, \"summary.csv\"), index=False)\n",
        "    return row\n"
      ],
      "metadata": {
        "id": "zJyLX3tn9Iv_"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replication on Original Dataset (SST-2)\n",
        "This section reproduces the LoRA results on the **SST-2** dataset to verify reproducibility of the original paper.  \n"
      ],
      "metadata": {
        "id": "5iIqie0p0u5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rep_sst2 = run_lora_experiment(\"sst2\", rank=8, epochs=CFG.epochs, lr=CFG.lr, batch_size=CFG.batch_size)\n",
        "pd.DataFrame([rep_sst2])\n"
      ],
      "metadata": {
        "id": "DY4xs0z_9Pru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Extension to New Datasets\n",
        "We extend LoRA fine-tuning to new text classification datasets (**IMDB**, **AG News**)  \n",
        "and a synthetic **IMDB-Mini-Paraphrased** dataset generated via back-translation.\n"
      ],
      "metadata": {
        "id": "BSLJZNq0E5Lg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üåç Extension: IMDB, AG News (rank = 8)\n"
      ],
      "metadata": {
        "id": "JbHN1sKk9T7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "for ds in [\"imdb\",\"ag_news\"]:\n",
        "    r = run_lora_experiment(ds, rank=8, epochs=CFG.epochs, lr=CFG.lr, batch_size=CFG.batch_size)\n",
        "    rows.append(r)\n",
        "\n",
        "ext_df = pd.DataFrame(rows)\n",
        "ext_df\n"
      ],
      "metadata": {
        "id": "wgyrthoa9Vj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, pandas as pd, pathlib, time\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "#  Load IMDB\n",
        "imdb = load_dataset(\"imdb\")\n",
        "train_texts = imdb[\"train\"][\"text\"][:2000]\n",
        "train_labels = imdb[\"train\"][\"label\"][:2000]\n",
        "test_texts  = imdb[\"test\"][\"text\"][:500]\n",
        "test_labels = imdb[\"test\"][\"label\"][:500]\n",
        "\n",
        "#  Translation models + tokenizers\n",
        "src_model_id = \"Helsinki-NLP/opus-mt-en-fr\"\n",
        "tgt_model_id = \"Helsinki-NLP/opus-mt-fr-en\"\n",
        "tok_en_fr = MarianTokenizer.from_pretrained(src_model_id)\n",
        "tok_fr_en = MarianTokenizer.from_pretrained(tgt_model_id)\n",
        "mod_en_fr = MarianMTModel.from_pretrained(src_model_id).to(device).eval()\n",
        "mod_fr_en = MarianMTModel.from_pretrained(tgt_model_id).to(device).eval()\n",
        "\n",
        "def backtranslate(texts, batch_size=16, max_len=128, max_new=96):\n",
        "    out = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Back-translation\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "\n",
        "        # EN -> FR\n",
        "        en_inputs = tok_en_fr(batch, return_tensors=\"pt\", padding=True,\n",
        "                              truncation=True, max_length=max_len).to(device)\n",
        "        with torch.no_grad():\n",
        "            fr_ids = mod_en_fr.generate(\n",
        "                **en_inputs, max_new_tokens=max_new, num_beams=1, do_sample=False\n",
        "            )\n",
        "        fr_texts = tok_en_fr.batch_decode(fr_ids, skip_special_tokens=True)\n",
        "\n",
        "        # FR -> EN\n",
        "        fr_inputs = tok_fr_en(fr_texts, return_tensors=\"pt\", padding=True,\n",
        "                              truncation=True, max_length=max_len).to(device)\n",
        "        with torch.no_grad():\n",
        "            en_ids = mod_fr_en.generate(\n",
        "                **fr_inputs, max_new_tokens=max_new, num_beams=1, do_sample=False\n",
        "            )\n",
        "        out.extend(tok_fr_en.batch_decode(en_ids, skip_special_tokens=True))\n",
        "\n",
        "        # optional: free memory\n",
        "        torch.cuda.empty_cache() if device == \"cuda\" else None\n",
        "    return out\n",
        "\n",
        "N = 300  # for faster outputs else let it be 0\n",
        "paraphrased = backtranslate(train_texts[:N], batch_size=16, max_len=128, max_new=96)\n",
        "\n",
        "train_texts_paraphrased = paraphrased + train_texts[:N]\n",
        "train_labels_paraphrased = train_labels[:N] + train_labels[:N]\n",
        "\n",
        "mini_df  = pd.DataFrame({\"text\": train_texts_paraphrased, \"label\": train_labels_paraphrased})\n",
        "test_df  = pd.DataFrame({\"text\": test_texts, \"label\": test_labels})\n",
        "mini_ds  = Dataset.from_pandas(mini_df)\n",
        "test_ds  = Dataset.from_pandas(test_df)\n",
        "imdb_mini = DatasetDict({\"train\": mini_ds, \"test\": test_ds})\n",
        "\n",
        "out_dir = pathlib.Path(\"data_notes/imdb_mini\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "mini_df.to_csv(out_dir/\"train.csv\", index=False)\n",
        "test_df.to_csv(out_dir/\"test.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Synthetic IMDB-Mini-Paraphrased created: {len(mini_ds)} training examples\")\n"
      ],
      "metadata": {
        "id": "T31FUvzoFDdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reuse the same run_lora_experiment by temporarily registering the dataset\n",
        "from datasets import DatasetDict\n",
        "imdb_dict = DatasetDict({\"train\": imdb_mini[\"train\"], \"validation\": imdb_mini[\"test\"], \"test\": imdb_mini[\"test\"]})\n",
        "\n",
        "# Monkey-patch loader just for this run\n",
        "DATASETS[\"imdb_mini\"] = {\"hf\": (\"imdb\",), \"text\": [\"text\"], \"label\": [\"label\"], \"test\": \"test\"}\n",
        "\n",
        "# Call the experiment\n",
        "res_imdb_mini = run_lora_experiment(\"imdb_mini\", rank=8, epochs=CFG.epochs, lr=CFG.lr, batch_size=CFG.batch_size)\n",
        "pd.DataFrame([res_imdb_mini])\n"
      ],
      "metadata": {
        "id": "7dhB3M_4FIeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-ZFJ0UAp1ZwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Rank Ablation (r = 4, 8, 16) on SST-2\n"
      ],
      "metadata": {
        "id": "ipyWk3U3Dy1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect all summary.csv files into one dataframe\n",
        "all_rows = []\n",
        "for p in pathlib.Path(CFG.out_dir).glob(\"*_r*/summary.csv\"):\n",
        "    df = pd.read_csv(p)\n",
        "    all_rows.append(df)\n",
        "summary = pd.concat(all_rows, ignore_index=True) if all_rows else pd.DataFrame()\n",
        "summary.sort_values([\"dataset\",\"rank\"], inplace=True)\n",
        "display(summary)\n",
        "\n",
        "# Simple accuracy bar plot (no specific colors per your tools constraints are irrelevant here; this is a notebook)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(summary[\"dataset\"], summary[\"accuracy\"])\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"LoRA (r=8) accuracy across datasets\")\n",
        "plt.xticks(rotation=15)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DFeba3KbDsUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abl_rows = []\n",
        "for r in [4,8,16]:\n",
        "    res = run_lora_experiment(\"sst2\", rank=r, epochs=CFG.epochs, lr=CFG.lr, batch_size=CFG.batch_size)\n",
        "    abl_rows.append(res)\n",
        "abl_df = pd.DataFrame(abl_rows)\n",
        "display(abl_df)\n",
        "\n",
        "plt.figure(figsize=(5,3))\n",
        "plt.plot(abl_df[\"rank\"], abl_df[\"accuracy\"], marker=\"o\")\n",
        "plt.xlabel(\"LoRA rank (r)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"SST-2 accuracy vs LoRA rank\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GyBrO-PtD3H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removed all outputs as my free-of-charge resources allowance exhausted during my run. I have added the ouput in the report."
      ],
      "metadata": {
        "id": "BYVgbt7G1cda"
      }
    }
  ]
}